version: "3.2"

networks:
  default:
    external:
      name: merlin_net
services:
  #HDFS and Spark Name Node Service
  namenode:
    image: 52.0.211.45:5000/merlin-hadoop-namenode:dev
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
      - "8088:8088"
      - "50070:50070"
      - "9000:9000"
#      - "8042:8042"
      - "8020:8020"
  #HDFS Data Nodes
  datanode:
    image: 52.0.211.45:5000/merlin-hadoop-base:dev
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
  #Hadoop/spark workers on Yarn
  sparkworker:
    image: 52.0.211.45:5000/merlin-hadoop-base:dev
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes:
      - $PWD/merlin-etl_mount/share:/share
      - $PWD/merlin-etl_mount/ingest:/ingest
    ports:
      - "8042:8042"
    command: "/root/hadoop_entrypoint.sh -spark_worker"
  #Hadoop/spark staging server
  stagingserver:
    image: 52.0.211.45:5000/merlin-hadoop-base:dev
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    volumes:
      - $PWD/merlin-etl_mount/share:/share
      - $PWD/merlin-etl_mount/ingest:/ingest
    command: "/root/hadoop_entrypoint.sh"

